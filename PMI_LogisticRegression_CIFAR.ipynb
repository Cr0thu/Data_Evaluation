{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.linalg import sqrtm\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torchvision.models as models\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image0 = np.load(\"/gfshome/cifar5m-part0.npz\")\n",
    "# image1 = np.load(\"/gfshome/cifar5m-part1.npz\")\n",
    "# X_tensor = torch.concatenate([torch.tensor(image0['X'], device = device).permute(0, 3, 1, 2), torch.tensor(image1['X'], device = device).permute(0, 3, 1, 2)], dim=0)\n",
    "# Y_tensor = torch.concatenate([torch.tensor(image0['Y'], device = device), torch.tensor(image1['Y'], device = device)], dim=0)\n",
    "# X_tensor = torch.tensor(image1['X'], device = device).permute(0, 3, 1, 2)\n",
    "# Y_tensor = image1['Y']\n",
    "# mask_0 = (Y_tensor == 0)\n",
    "# mask_1 = (Y_tensor == 1)\n",
    "# X_train_tensor_0 = X_tensor[mask_0][:100000,:,:,:]/256\n",
    "# X_train_tensor_1 = X_tensor[mask_1][:100000,:,:,:]/256\n",
    "# X_test_tensor_0 = X_tensor[mask_0][100000:150000,:,:,:]/256\n",
    "# X_test_tensor_1 = X_tensor[mask_1][:50000,:,:,:]/256\n",
    "# X_train_tensor_0 = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))(X_train_tensor_0)\n",
    "# X_train_tensor_1 = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))(X_train_tensor_1)\n",
    "# X_test_tensor_0 = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))(X_test_tensor_0)\n",
    "# X_test_tensor_1 = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))(X_test_tensor_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LogisticRegression(torch.nn.Module):\n",
    "#     def __init__(self, input_dim, fit_intercept=True, C=1.0, max_iter=100):\n",
    "#         super(LogisticRegression, self).__init__()\n",
    "#         self.fit_intercept = fit_intercept\n",
    "#         self.C = C\n",
    "#         self.max_iter = max_iter\n",
    "        \n",
    "#         if self.fit_intercept:\n",
    "#             self.linear = nn.Linear(input_dim, 1)\n",
    "#         else:\n",
    "#             self.linear = nn.Linear(input_dim, 1, bias=False)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         return torch.sigmoid(self.linear(x))\n",
    "    \n",
    "#     def fit(self, X, y):\n",
    "#         X = torch.tensor(X, dtype=torch.float32)\n",
    "#         y = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "        \n",
    "#         criterion = nn.BCELoss()\n",
    "#         optimizer = optim.SGD(self.parameters(), lr=1e-3)\n",
    "        \n",
    "#         for epoch in range(self.max_iter):\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = self.forward(X)\n",
    "#             loss = criterion(outputs, y)\n",
    "            \n",
    "#             l2_lambda = 1.0 / self.C\n",
    "#             l2_reg = sum(param.pow(2.0).sum() for param in self.parameters())\n",
    "#             loss = loss + l2_lambda * l2_reg\n",
    "            \n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "        \n",
    "#         self.coef_ = self.linear.weight.detach().cpu().numpy()\n",
    "#         if self.fit_intercept:\n",
    "#             self.intercept_ = self.linear.bias.detach().cpu().numpy()\n",
    "#         else:\n",
    "#             self.intercept_ = None\n",
    "\n",
    "#     def predict_proba(self, X):\n",
    "#         X = torch.tensor(X, dtype=torch.float32)\n",
    "#         return self.forward(X).detach().cpu().numpy()\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         proba = self.predict_proba(X)\n",
    "#         return (proba >= 0.5).astype(int)\n",
    "    \n",
    "#     def score(self, X, y):\n",
    "#         X = torch.tensor(X, dtype=torch.float32)\n",
    "#         y = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "#         y_pred = self.predict(X)\n",
    "#         accuracy = (y_pred == y.cpu().numpy()).mean()\n",
    "#         return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_large = 400\n",
    "N_small = 100\n",
    "mu = 1.0\n",
    "var = 2.0\n",
    "D = 3\n",
    "T = 3\n",
    "test_size_L, test_size_H, test_size_step = 30, 50, 10\n",
    "train_size = 800\n",
    "candidate_L, candidate_H, candidate_step = 200, 600, 200\n",
    "top_candidate = [3, 9, 27]\n",
    "penalty = 5\n",
    "eps = 0.01\n",
    "width = 0.15\n",
    "gap = 0.05\n",
    "MCsample = 500\n",
    "sigma = 1e-1\n",
    "p = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet18 = models.resnet18(pretrained=True)\n",
    "# resnet18.fc = torch.nn.Identity()\n",
    "# resnet18.to(device).eval()\n",
    "# print(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     X_train_0_embedding = resnet18(X_train_tensor_0)\n",
    "#     X_train_1_embedding = resnet18(X_train_tensor_1)\n",
    "#     X_test_0_embedding = resnet18(X_test_tensor_0)\n",
    "    # X_test_1_embedding = resnet18(X_test_tensor_1)\n",
    "#     torch.save(X_train_0_embedding, \"/gfshome/train_0.pt\")\n",
    "#     torch.save(X_train_1_embedding, \"/gfshome/train_1.pt\")\n",
    "#     torch.save(X_test_0_embedding, \"/gfshome/test_0.pt\")\n",
    "    # torch.save(X_test_1_embedding, \"/gfshome/test_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_0_embedding = torch.load(\"/gfshome/train_0.pt\")\n",
    "X_train_1_embedding = torch.load(\"/gfshome/train_1.pt\")\n",
    "X_train_0_embedding = torch.concatenate([X_train_0_embedding, torch.ones(X_train_0_embedding.size()[0],1).to(device)], dim=1)\n",
    "X_train_1_embedding = torch.concatenate([X_train_1_embedding, torch.ones(X_train_1_embedding.size()[0],1).to(device)], dim=1)\n",
    "X_test_0_embedding = torch.load(\"/gfshome/test_0.pt\")\n",
    "X_test_1_embedding = torch.load(\"/gfshome/test_1.pt\")\n",
    "P_x = torch.concatenate([X_test_0_embedding[:p], X_test_1_embedding[:p]])\n",
    "P_x = torch.concatenate([P_x, torch.ones(P_x.size()[0],1).to(device)], dim=1)\n",
    "P_y = torch.concatenate([torch.zeros(p), torch.ones(p)]).to(device)\n",
    "# allocated_memory = torch.cuda.memory_allocated()\n",
    "# print(f\"Already allocated: {allocated_memory / 1024 ** 2} MB\")\n",
    "# print(X_test_0_embedding.size())\n",
    "input_dim = P_x.size()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    def __init__(self, X, y, score, post_loss, base_loss, smooth, post_acc, base_acc, noise_ratio, label_ratio):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.score = score\n",
    "        self.post_loss = post_loss\n",
    "        self.base_loss = base_loss\n",
    "        self.smooth = smooth\n",
    "        self.post_acc = post_acc\n",
    "        self.base_acc = base_acc\n",
    "        self.noise_ratio = noise_ratio\n",
    "        self.label_ratio = label_ratio\n",
    "\n",
    "class result:\n",
    "    def __init__(self, score_loss, score_acc, post_loss_loss, post_loss_acc, base_loss_loss, base_loss_acc, smooth_loss, smooth_acc, post_acc_loss, post_acc_acc, base_acc_loss, base_acc_acc):\n",
    "        self.score_loss = score_loss\n",
    "        self.score_acc = score_acc\n",
    "        self.post_loss_loss = post_loss_loss\n",
    "        self.post_loss_acc = post_loss_acc\n",
    "        self.base_loss_loss = base_loss_loss\n",
    "        self.base_loss_acc = base_loss_acc\n",
    "        self.smooth_loss = smooth_loss\n",
    "        self.smooth_acc = smooth_acc\n",
    "        self.post_acc_loss = post_acc_loss\n",
    "        self.post_acc_acc = post_acc_acc\n",
    "        self.base_acc_loss = base_acc_loss\n",
    "        self.base_acc_acc = base_acc_acc\n",
    "\n",
    "    def add(self, another):        \n",
    "        self.score_loss += another.score_loss\n",
    "        self.score_acc += another.score_acc\n",
    "        self.post_loss_loss += another.post_loss_loss\n",
    "        self.post_loss_acc += another.post_loss_acc\n",
    "        self.base_loss_loss += another.base_loss_loss\n",
    "        self.base_loss_acc += another.base_loss_acc\n",
    "        self.smooth_loss += another.smooth_loss\n",
    "        self.smooth_acc += another.smooth_acc\n",
    "        self.post_acc_loss += another.post_acc_loss\n",
    "        self.post_acc_acc += another.post_acc_acc\n",
    "        self.base_acc_loss += another.base_acc_loss\n",
    "        self.base_acc_acc += another.base_acc_acc\n",
    "\n",
    "    def divide(self, num):        \n",
    "        self.score_loss = self.score_loss / num\n",
    "        self.score_acc = self.score_acc / num\n",
    "        self.post_loss_loss = self.post_loss_loss / num\n",
    "        self.post_loss_acc = self.post_loss_acc / num\n",
    "        self.base_loss_loss = self.base_loss_loss / num\n",
    "        self.base_loss_acc = self.base_loss_acc / num\n",
    "        self.smooth_loss = self.smooth_loss / num\n",
    "        self.smooth_acc = self.smooth_acc / num\n",
    "        self.post_acc_loss = self.post_acc_loss / num\n",
    "        self.post_acc_acc = self.post_acc_acc / num\n",
    "        self.base_acc_loss = self.base_acc_loss / num\n",
    "        self.base_acc_acc = self.base_acc_acc / num\n",
    "\n",
    "    def getacc(self):\n",
    "        return [self.score_acc, self.post_loss_acc, self.base_loss_acc, self.smooth_acc, self.post_acc_acc, self.base_acc_acc]\n",
    "    \n",
    "    def getloss(self):\n",
    "        return [self.score_loss, self.post_loss_loss, self.base_loss_loss, self.smooth_loss, self.post_acc_loss, self.base_acc_loss]\n",
    "\n",
    "#    def self_print(self, num):\n",
    "#        print(\"score: \", self.score_loss/num, self.score_acc/num, \"\\nlog loss:\", self.err_loss/num, self.err_acc/num, \"\\naccuracy:\", self.acc_loss/num, self.acc_acc/num)\n",
    "\n",
    "\n",
    "class summary:\n",
    "    def __init__(self, percentage_smooth_post, percentage_score_post, percentage_post_base_loss, percentage_post_base_acc, total_result):\n",
    "        self.percentage_smooth_post = percentage_smooth_post\n",
    "        self.percentage_score_post = percentage_score_post\n",
    "        self.percentage_post_base_loss = percentage_post_base_loss\n",
    "        self.percentage_post_base_acc = percentage_post_base_acc\n",
    "        self.total_result = total_result\n",
    "        \n",
    "    def add(self, another):\n",
    "        self.percentage_smooth_post = np.add(self.percentage_smooth_post, another.percentage_smooth_post)\n",
    "        self.percentage_score_post = np.add(self.percentage_score_post, another.percentage_score_post)\n",
    "        self.percentage_post_base_loss = np.add(self.percentage_post_base_loss, another.percentage_post_base_loss)\n",
    "        self.percentage_post_base_acc = np.add(self.percentage_post_base_acc, another.percentage_post_base_acc)\n",
    "        self.total_result.add(another.total_result)\n",
    "        \n",
    "    def divide(self, num):\n",
    "        self.percentage_smooth_post = self.percentage_smooth_post / num\n",
    "        self.percentage_score_post = self.percentage_score_post / num \n",
    "        self.percentage_post_base_loss = self.percentage_post_base_loss / num \n",
    "        self.percentage_post_base_acc = self.percentage_post_base_acc / num\n",
    "        self.total_result.divide(num)\n",
    "\n",
    "    def getacc(self):\n",
    "        return self.total_result.getacc()\n",
    "    \n",
    "    def getloss(self):\n",
    "        return self.total_result.getloss()\n",
    "\n",
    "#    def self_print_percentage(self, num):\n",
    "#        print(self.percentage_loss_err/num, self.percentage_loss_acc/num, self.percentage_acc_err/num, self.percentage_acc_acc/num)\n",
    "\n",
    "#    def self_print_result(self, num):\n",
    "#        self.total_result.self_print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + torch.exp(-z))\n",
    "\n",
    "def generate_data_cifar10(num, dataset, label):\n",
    "    indices = torch.randperm(dataset.size(0))[:num]\n",
    "    X = dataset[indices]\n",
    "    y = torch.ones(num) * label\n",
    "    y = torch.tensor(y, dtype=torch.int)\n",
    "    y = y.to(device)\n",
    "    return X, y\n",
    "\n",
    "def generate_dataset_cifar10(N, noise):\n",
    "    N_0 = random.randint(1,N-1)\n",
    "    X0, y0 = generate_data_cifar10(N_0, X_train_0_embedding, 0)\n",
    "    X1, y1 = generate_data_cifar10(N-N_0, X_train_1_embedding, 1)\n",
    "\n",
    "    X = torch.concatenate((X0, X1), axis = 0)\n",
    "    y = torch.concatenate((y0, y1), axis = 0)\n",
    "    for i in range(N):\n",
    "        flip = torch.bernoulli(torch.tensor(noise)).item()\n",
    "        if flip == 1:\n",
    "            y[i] = 1 - y[i]\n",
    "    return N_0/N, X, y\n",
    "\n",
    "def generate_train_cifar10(train_size, train_number):\n",
    "    train_dataset = []\n",
    "    for i in range(train_number):\n",
    "        noise_level = random.randint(1, 30)\n",
    "        # noise_level = 0\n",
    "        not_found = True\n",
    "        while not_found:\n",
    "            label_ratio, train_X, train_y = generate_dataset_cifar10(train_size, eps * noise_level)\n",
    "            if torch.sum(train_y)>0 and torch.sum(train_y)<train_size:\n",
    "                not_found = False\n",
    "        train_dataset.append(dataset(train_X, train_y, 0, 0, 0, 0, 0, 0, noise_level*eps, label_ratio))\n",
    "    return train_dataset\n",
    "\n",
    "def subsample(X, y, size):\n",
    "    perm = torch.randperm(len(y))\n",
    "    sample_X = X[perm[:size]]\n",
    "    sample_y = y[perm[:size]]\n",
    "    return sample_X, sample_y\n",
    "\n",
    "def compute_hessian(mu, X):\n",
    "    m = X.size()[1]\n",
    "    res = torch.eye(m)\n",
    "    res = res.to(device)\n",
    "    for i in range(X.size()[0]):\n",
    "        xi = X[i:i+1, :]\n",
    "        sigm = sigmoid(torch.dot(xi[0], mu[0]))\n",
    "        res += torch.matmul(xi.t(), xi) * (sigm * (1 - sigm)).item()\n",
    "    return res\n",
    "\n",
    "def compute_score(mu0, Q0, lg0, mu1, Q1, lg1, mu2, Q2, lg2):\n",
    "    Q = Q1 + Q2 - Q0\n",
    "    Q_inv = torch.inverse(Q)\n",
    "    mu = torch.matmul(Q_inv, torch.matmul(Q1, mu1) + torch.matmul(Q2, mu2) - torch.matmul(Q0, mu0))\n",
    "\n",
    "    # Q_numpy = Q.detach().cpu().numpy()\n",
    "    # eigenvalue, _ = np.linalg.eigh(Q_numpy)\n",
    "    # eigenvalue = torch.tensor(eigenvalue)\n",
    "    # lg12 = torch.sum(torch.log(eigenvalue))\n",
    "    eigenvalue, _ = torch.linalg.eigh(Q)\n",
    "    lg12 = torch.sum(torch.log(eigenvalue))\n",
    "\n",
    "    lg = lg1+lg2-lg12-lg0\n",
    "\n",
    "    sqr = torch.matmul(mu.T, torch.matmul(Q, mu)) - torch.matmul(mu1.T, torch.matmul(Q1, mu1)) - torch.matmul(mu2.T, torch.matmul(Q2, mu2)) + torch.matmul(mu0.T, torch.matmul(Q0, mu0))\n",
    "\n",
    "    score = 0.5 * (lg + sqr)\n",
    "    # print(lg.item(), sqr.item())\n",
    "    \n",
    "    return score.item()\n",
    "\n",
    "def compute_data_score_err(mu_test, Q_test, test_X, test_y, train_X, train_y, lg2):\n",
    "    test_N = test_y.size()[0]\n",
    "    M = test_X.size()[1]\n",
    "\n",
    "    mu0 = torch.zeros((1, M))\n",
    "    mu0 = mu0.to(device)\n",
    "    Q0 = torch.eye(M)/penalty\n",
    "    Q0 = Q0.to(device)\n",
    "    lg0 = -M * torch.log(torch.tensor(penalty))\n",
    "    \n",
    "    train = LogisticRegression(fit_intercept = False, C = penalty, max_iter=5000).fit(train_X.cpu(), train_y.cpu())\n",
    "    mu_train = torch.tensor(train.coef_, dtype=torch.float32, device=device)\n",
    "    mu_train_numpy = mu_train.detach().squeeze().cpu().numpy()\n",
    "\n",
    "    Q_train = compute_hessian(mu_train, train_X)\n",
    "    Q_train_inverse = torch.inverse(Q_train)\n",
    "    Q_numpy = Q_train_inverse.detach().cpu().numpy()\n",
    "\n",
    "    # Q_train_numpy = Q_train.detach().cpu().numpy()\n",
    "    # eigenvalue, _ = np.linalg.eigh(Q_train_numpy)\n",
    "    # eigenvalue = torch.tensor(eigenvalue)\n",
    "    # lg1 = torch.sum(torch.log(eigenvalue))\n",
    "    eigenvalue, _ = torch.linalg.eigh(Q_train)\n",
    "    lg1 = torch.sum(torch.log(eigenvalue))\n",
    "\n",
    "    score = compute_score(mu0.t(), Q0, lg0, mu_train.t(), Q_train, lg1, mu_test.t(), Q_test, lg2)\n",
    "    # print(score)\n",
    "\n",
    "    # log_loss = torch.zeros(test_N)\n",
    "    # log_loss = log_loss.to(device)\n",
    "    # pred = torch.tensor(train.predict_proba(test_X.cpu()), dtype=torch.float32, device=device)\n",
    "    \n",
    "#     for j in range(test_N):\n",
    "#         a = test_y[j]\n",
    "#         b = pred[j,1]\n",
    "# #        print(a,b)\n",
    "#         log_loss[j] = a * torch.log(b) + (1 - a) * torch.log(1 - b)\n",
    "        \n",
    "#     test_err = torch.sum(log_loss)/test_N\n",
    "#     test_acc = train.score(test_X.cpu(), test_y.cpu())\n",
    "\n",
    "    # z = 2 * test_y - 1\n",
    "    # z = z.view(-1,1)\n",
    "    test_y = test_y.float()\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    base_predictive = sigmoid(torch.matmul(test_X, mu_train.t())).squeeze()\n",
    "    base_predictions = (base_predictive >= 0.5).float()\n",
    "    base_loss = criterion(base_predictive, test_y)\n",
    "    base_acc = (base_predictions == test_y).float().mean()\n",
    "\n",
    "    # mvn = torch.distributions.MultivariateNormal(mu_train, Q_train_inverse)\n",
    "    # theta_samples = mvn.sample((MCsample, ))\n",
    "    # print(mu_train_numpy.shape)\n",
    "    theta_samples = np.random.multivariate_normal(mu_train_numpy, Q_numpy, MCsample, 'warn')\n",
    "    theta_samples = torch.tensor(theta_samples).to(device)\n",
    "    theta_samples = theta_samples.float()\n",
    "\n",
    "    post_predictive = torch.mean(sigmoid(torch.matmul(test_X, theta_samples.t())), dim=1)\n",
    "    post_predictions = (post_predictive >= 0.5).float()\n",
    "    post_loss = criterion(post_predictive, test_y)\n",
    "    post_acc = (post_predictions == test_y).float().mean()\n",
    "\n",
    "    Q_numpy_smooth = sigma * np.eye(M)\n",
    "    theta_samples_smooth = np.random.multivariate_normal(mu_train_numpy, Q_numpy_smooth, MCsample, 'warn')\n",
    "    theta_samples_smooth = torch.tensor(theta_samples_smooth).to(device)\n",
    "    theta_samples_smooth = theta_samples_smooth.float()\n",
    "\n",
    "    smooth_predictive = torch.mean(sigmoid(torch.matmul(test_X, theta_samples_smooth.t())), dim=1)\n",
    "    # smooth_predictions = (smooth_predictive >= 0.5).float()\n",
    "    smooth_loss = criterion(smooth_predictive, test_y)\n",
    "    # smooth_acc = (smooth_predictions == test_y).float().mean()\n",
    "    \n",
    "    return score, base_loss.item(), base_acc, post_loss.item(), post_acc, smooth_loss.item()\n",
    "\n",
    "def get_err_score(train_data, test_X, test_y, train_number):\n",
    "    test = LogisticRegression(fit_intercept = False, C = penalty, max_iter=5000).fit(test_X.cpu(), test_y.cpu())\n",
    "    mu_test = torch.tensor(test.coef_, dtype=torch.float32, device=device)\n",
    "    Q_test = compute_hessian(mu_test, test_X)\n",
    "    # Q_test_numpy = Q_test.detach().cpu().numpy()\n",
    "    # eigenvalue, _ = np.linalg.eigh(Q_test_numpy)\n",
    "    # eigenvalue = torch.tensor(eigenvalue)\n",
    "    # lg2 = torch.sum(torch.log(eigenvalue))\n",
    "    eigenvalue, _ = torch.linalg.eigh(Q_test)\n",
    "    lg2 = torch.sum(torch.log(eigenvalue))\n",
    "\n",
    "    for i in range(train_number):\n",
    "        train_data[i].score, train_data[i].base_loss, train_data[i].base_acc, train_data[i].post_loss, train_data[i].post_acc, train_data[i].smooth = compute_data_score_err(mu_test, Q_test, test_X, test_y, train_data[i].X, train_data[i].y, lg2)\n",
    "\n",
    "def compute_data_err_acc(test_X, test_y, train_X, train_y):\n",
    "    test_N = test_y.size()[0]\n",
    "    train = LogisticRegression(fit_intercept = False, C = penalty, max_iter=5000).fit(train_X.cpu(), train_y.cpu())\n",
    "    mu_train = torch.tensor(train.coef_, dtype=torch.float32, device=device)\n",
    "    mu_train_numpy = mu_train.detach().squeeze().cpu().numpy()\n",
    "    Q_train = compute_hessian(mu_train, train_X)\n",
    "    Q_train_inverse = torch.inverse(Q_train)\n",
    "    Q_numpy = Q_train_inverse.detach().cpu().numpy()\n",
    "\n",
    "#     log_loss = torch.zeros(test_N)\n",
    "#     log_loss = log_loss.to(device)\n",
    "#     pred = torch.tensor(train.predict_proba(test_X.cpu()), dtype=torch.float32, device=device)\n",
    "\n",
    "#     for j in range(test_N):\n",
    "#         a = test_y[j]\n",
    "#         b = pred[j,1]\n",
    "# #        print(a,b)\n",
    "#         log_loss[j] = a * torch.log(b) + (1 - a) * torch.log(1 - b)\n",
    "        \n",
    "#     test_err = torch.sum(log_loss)/test_N\n",
    "#     test_acc = train.score(test_X.cpu(), test_y.cpu())\n",
    "    test_y = test_y.float()\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    base_predictive = sigmoid(torch.matmul(test_X, mu_train.t())).squeeze()\n",
    "    base_predictions = (base_predictive >= 0.5).float()\n",
    "    base_loss = criterion(base_predictive, test_y)\n",
    "    base_acc = (base_predictions == test_y).float().mean()\n",
    "\n",
    "    theta_samples = np.random.multivariate_normal(mu_train_numpy, Q_numpy, MCsample, 'warn')\n",
    "    theta_samples = torch.tensor(theta_samples).to(device)\n",
    "    theta_samples = theta_samples.float()\n",
    "\n",
    "    # z = 2 * test_y - 1\n",
    "    # z = z.view(-1,1)\n",
    "    # print(z.view(-1, 1) * torch.matmul(test_X, theta_samples.t()))\n",
    "    # post_predict = (1 / MCsample) * torch.sum(sigmoid(torch.matmul(test_X, theta_samples.t())), dim=1)\n",
    "\n",
    "    post_predictive = torch.mean(sigmoid(torch.matmul(test_X, theta_samples.t())), dim=1)\n",
    "    post_predictions = (post_predictive >= 0.5).float()\n",
    "    post_loss = criterion(post_predictive, test_y)\n",
    "    post_acc = (post_predictions == test_y).float().mean()\n",
    "\n",
    "\n",
    "    # loss = criterion(post_predict, test_y.float())\n",
    "    # predicted = (post_predict > 0.5).float()\n",
    "    # accuracy = (predicted == test_y.float()).float().mean()\n",
    "\n",
    "    # print(post_predict)\n",
    "    # sys.exit()\n",
    "    return base_loss.item(), base_acc, post_loss.item(), post_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plot(x_label, a_L, a_H, label1, label2, save_name):\n",
    "#    print(width, gap)\n",
    "    x = np.array(range(len(x_label))) * (width + gap)\n",
    "#    print(e_L, e_H)\n",
    "    acc_L = np.ones(len(x_label))*100 - np.array(a_H) \n",
    "    acc_H = np.ones(len(x_label))*100 - np.array(a_L)\n",
    "    plt.rcParams['figure.figsize'] = [(width + gap) * (len(x_label) + 1), 4]\n",
    "    plt.bar(x, acc_L, color='sandybrown', width = width, label = label1)\n",
    "    plt.bar(x, np.ones(len(x_label))*100 - acc_H, bottom= acc_H, color='cornflowerblue', width = width, label = label2)\n",
    "    plt.bar(x, acc_H - acc_L, bottom= acc_L, color='lightgray', width = width, label = 'Ties')\n",
    "    plt.xticks(x, x_label, rotation='vertical')\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc=4)\n",
    "    plt.savefig(\"figure/\"+save_name+\".pdf\", format=\"pdf\", dpi=200) #add legend\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [1:16:02<00:00, 1520.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$3, 30, 200$ & 0.9045 & 0.8741 & 0.8692 & 0.8841 & 0.8637 & 0.8684 \\\\ \n",
      "$3, 30, 200$ & 0.9043 & 0.8741 & 0.8692 & 0.8841 & 0.8638 & 0.8682 \\\\ \n",
      "$3, 30, 200$ & 0.0001 & 0.0001 & 0.0001 & 0.0000 & -0.0000 & 0.0002 & \n",
      "$3, 30, 200$ & 0.4747 & 0.4389 & 0.4469 & 0.4851 & 0.4376 & 0.4356 \\\\ \n",
      "$3, 30, 200$ & 0.2554 & 0.3195 & 0.3296 & 0.3090 & 0.3368 & 0.3278 \\\\ \n",
      "$3, 30, 200$ & 0.2193 & 0.1194 & 0.1173 & 0.1761 & 0.1009 & 0.1077 & \n",
      "$9, 30, 200$ & 0.9310 & 0.9260 & 0.9256 & 0.9289 & 0.9275 & 0.9284 \\\\ \n",
      "$9, 30, 200$ & 0.9310 & 0.9259 & 0.9258 & 0.9287 & 0.9272 & 0.9285 \\\\ \n",
      "$9, 30, 200$ & 0.0000 & 0.0002 & -0.0002 & 0.0002 & 0.0002 & -0.0002 & \n",
      "$9, 30, 200$ & 0.1828 & 0.1964 & 0.2022 & 0.1892 & 0.1947 & 0.1948 \\\\ \n",
      "$9, 30, 200$ & 0.1817 & 0.2045 & 0.2127 & 0.1961 & 0.2039 & 0.2046 \\\\ \n",
      "$9, 30, 200$ & 0.0011 & -0.0082 & -0.0105 & -0.0069 & -0.0093 & -0.0098 & \n",
      "$27, 30, 200$ & 0.9429 & 0.9418 & 0.9418 & 0.9421 & 0.9412 & 0.9409 \\\\ \n",
      "$27, 30, 200$ & 0.9430 & 0.9419 & 0.9418 & 0.9421 & 0.9413 & 0.9407 \\\\ \n",
      "$27, 30, 200$ & -0.0000 & -0.0001 & 0.0000 & 0.0001 & -0.0000 & 0.0001 & \n",
      "$27, 30, 200$ & 0.1603 & 0.1782 & 0.1883 & 0.1760 & 0.1887 & 0.1850 \\\\ \n",
      "$27, 30, 200$ & 0.1646 & 0.1835 & 0.1939 & 0.1812 & 0.1943 & 0.1904 \\\\ \n",
      "$27, 30, 200$ & -0.0042 & -0.0053 & -0.0056 & -0.0052 & -0.0055 & -0.0054 & \n",
      "\n",
      "  _     ._   __/__   _ _  _  _ _/_   Recorded: 04:37:55  Samples:  785585\n",
      " /_//_/// /_\\ / //_// / //_'/ //     Duration: 4562.756  CPU time: 33784.074\n",
      "/   _/                      v4.6.2\n",
      "\n",
      "Program: /opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py --f=/root/.local/share/jupyter/runtime/kernel-v2-2092453Q2O2Iu4ipl9J.json\n",
      "\n",
      "4562.666 ZMQInteractiveShell.run_ast_nodes  IPython/core/interactiveshell.py:3367\n",
      "└─ 4562.664 <module>  ../../tmp/ipykernel_2097575/10442743.py:1\n",
      "   ├─ 3608.079 get_err_score  ../../tmp/ipykernel_2097575/428917728.py:150\n",
      "   │  └─ 3606.525 compute_data_score_err  ../../tmp/ipykernel_2097575/428917728.py:75\n",
      "   │     ├─ 1460.774 wrapper  sklearn/base.py:1457\n",
      "   │     │     [19 frames hidden]  sklearn, joblib, scipy\n",
      "   │     ├─ 1111.660 svd  numpy/linalg/linalg.py:1499\n",
      "   │     │     [2 frames hidden]  numpy\n",
      "   │     │        1108.654 [self]  numpy/linalg/linalg.py\n",
      "   │     ├─ 580.262 compute_hessian  ../../tmp/ipykernel_2097575/428917728.py:44\n",
      "   │     │  ├─ 204.551 [self]  ../../tmp/ipykernel_2097575/428917728.py\n",
      "   │     │  ├─ 162.715 sigmoid  ../../tmp/ipykernel_2097575/428917728.py:1\n",
      "   │     │  │  ├─ 72.472 [self]  ../../tmp/ipykernel_2097575/428917728.py\n",
      "   │     │  │  └─ 65.273 wrapped  torch/_tensor.py:34\n",
      "   │     │  │        [2 frames hidden]  torch\n",
      "   │     │  ├─ 53.071 _VariableFunctionsClass.dot  <built-in>\n",
      "   │     │  └─ 49.148 _VariableFunctionsClass.matmul  <built-in>\n",
      "   │     ├─ 280.236 [self]  ../../tmp/ipykernel_2097575/428917728.py\n",
      "   │     └─ 88.999 RandomState.multivariate_normal  <built-in>\n",
      "   ├─ 864.643 compute_data_err_acc  ../../tmp/ipykernel_2097575/428917728.py:164\n",
      "   │  ├─ 455.324 wrapper  sklearn/base.py:1457\n",
      "   │  │     [19 frames hidden]  sklearn, joblib, scipy\n",
      "   │  ├─ 327.786 compute_hessian  ../../tmp/ipykernel_2097575/428917728.py:44\n",
      "   │  │  ├─ 106.875 sigmoid  ../../tmp/ipykernel_2097575/428917728.py:1\n",
      "   │  │  └─ 103.834 [self]  ../../tmp/ipykernel_2097575/428917728.py\n",
      "   │  └─ 57.023 svd  numpy/linalg/linalg.py:1499\n",
      "   │        [2 frames hidden]  numpy\n",
      "   └─ 87.849 generate_train_cifar10  ../../tmp/ipykernel_2097575/428917728.py:25\n",
      "      └─ 87.050 generate_dataset_cifar10  ../../tmp/ipykernel_2097575/428917728.py:12\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pyinstrument import Profiler\n",
    "\n",
    "# profiler = Profiler()\n",
    "# profiler.start()\n",
    "x_label_all = []\n",
    "\n",
    "single_smooth_post_L_all = []\n",
    "single_smooth_post_H_all = []\n",
    "single_score_post_L_all = []\n",
    "single_score_post_H_all = []\n",
    "single_post_base_loss_L_all = []\n",
    "single_post_base_loss_H_all = []\n",
    "single_post_base_acc_L_all = []\n",
    "single_post_base_acc_H_all = []\n",
    "\n",
    "smooth_post_L_all = []\n",
    "smooth_post_H_all = []\n",
    "score_post_L_all = []\n",
    "score_post_H_all = []\n",
    "post_base_loss_L_all = []\n",
    "post_base_loss_H_all = []\n",
    "post_base_acc_L_all = []\n",
    "post_base_acc_H_all = []\n",
    "\n",
    "single_acc_all_all = []\n",
    "acc_all_all = []\n",
    "single_loss_all_all = []\n",
    "loss_all_all = []\n",
    "\n",
    "for test_size in range(test_size_L, test_size_H+1, test_size_step):\n",
    "    x_label = []\n",
    "\n",
    "    single_smooth_post_L = []\n",
    "    single_smooth_post_H = []\n",
    "    single_score_post_L = []\n",
    "    single_score_post_H = []\n",
    "    single_post_base_loss_L = []\n",
    "    single_post_base_loss_H = []\n",
    "    single_post_base_acc_L = []\n",
    "    single_post_base_acc_H= []\n",
    "\n",
    "    smooth_post_L = []\n",
    "    smooth_post_H = []\n",
    "    score_post_L = []\n",
    "    score_post_H = []\n",
    "    post_base_loss_L = []\n",
    "    post_base_loss_H = []\n",
    "    post_base_acc_L = []\n",
    "    post_base_acc_H = []\n",
    "\n",
    "    single_acc_all = []\n",
    "    acc_all = []\n",
    "    single_loss_all = []\n",
    "    loss_all = []\n",
    "\n",
    "    for num_candidate in range(candidate_L, candidate_H+1, candidate_step):\n",
    "        single_total_summary = []\n",
    "        total_summary = []\n",
    "        for i in range(len(top_candidate)):\n",
    "            single_total_summary.append(summary([0,0], [0,0], [0,0], [0,0], result(0,0,0,0,0,0,0,0,0,0,0,0)))\n",
    "            total_summary.append(summary([0,0], [0,0], [0,0], [0,0], result(0,0,0,0,0,0,0,0,0,0,0,0)))\n",
    "        for d in tqdm(range(D)):\n",
    "            test_X = P_x\n",
    "            test_y = P_y\n",
    "            test = LogisticRegression(fit_intercept = False, C = penalty, max_iter=5000).fit(test_X.cpu(), test_y.cpu())\n",
    "            acc = test.score(test_X.cpu(), test_y.cpu())\n",
    "            print(acc)\n",
    "            sys.exit()\n",
    "\n",
    "            train_data = generate_train_cifar10(train_size, num_candidate)\n",
    "            single_total_results = []\n",
    "            total_results = []\n",
    "            single_results = []\n",
    "            results = []\n",
    "            for i in range(len(top_candidate)):\n",
    "                single_total_results.append(result(0,0,0,0,0,0,0,0,0,0,0,0))\n",
    "                total_results.append(result(0,0,0,0,0,0,0,0,0,0,0,0))\n",
    "                single_results.append([])\n",
    "                results.append([])\n",
    "            for t in range(T):\n",
    "                not_found = True\n",
    "                while not_found:\n",
    "                    sample_test_X, sample_test_y = subsample(test_X, test_y, test_size)\n",
    "                    if torch.sum(sample_test_y)>0 and torch.sum(sample_test_y)<test_size:\n",
    "                        not_found = False\n",
    "                \n",
    "                get_err_score(train_data, sample_test_X, sample_test_y, num_candidate)\n",
    "                sorted_score = sorted(train_data, key=lambda x: x.score, reverse = True)\n",
    "                sorted_post_loss = sorted(train_data, key=lambda x: x.post_loss, reverse = False)\n",
    "                sorted_base_loss = sorted(train_data, key=lambda x: x.base_loss, reverse = False)\n",
    "                sorted_smooth = sorted(train_data, key=lambda x: x.smooth, reverse = False)\n",
    "                sorted_post_acc = sorted(train_data, key=lambda x: x.post_acc, reverse = True)\n",
    "                sorted_base_acc = sorted(train_data, key=lambda x: x.base_acc, reverse = True)\n",
    "\n",
    "                score_X = sorted_score[0].X\n",
    "                score_y = sorted_score[0].y\n",
    "                post_loss_X = sorted_post_loss[0].X\n",
    "                post_loss_y = sorted_post_loss[0].y\n",
    "                base_loss_X = sorted_base_loss[0].X\n",
    "                base_loss_y = sorted_base_loss[0].y\n",
    "                smooth_X = sorted_smooth[0].X\n",
    "                smooth_y = sorted_smooth[0].y\n",
    "                post_acc_X = sorted_post_acc[0].X\n",
    "                post_acc_y = sorted_post_acc[0].y\n",
    "                base_acc_X = sorted_base_acc[0].X\n",
    "                base_acc_y = sorted_base_acc[0].y\n",
    "\n",
    "                for j in range(1, top_candidate[-1]):\n",
    "                    score_X = torch.concatenate((score_X, sorted_score[j].X), axis = 0)\n",
    "                    score_y = torch.concatenate((score_y, sorted_score[j].y), axis = 0)\n",
    "                    post_loss_X = torch.concatenate((post_loss_X, sorted_post_loss[j].X), axis = 0)\n",
    "                    post_loss_y = torch.concatenate((post_loss_y, sorted_post_loss[j].y), axis = 0)\n",
    "                    base_loss_X = torch.concatenate((base_loss_X, sorted_base_loss[j].X), axis = 0)\n",
    "                    base_loss_y = torch.concatenate((base_loss_y, sorted_base_loss[j].y), axis = 0)\n",
    "                    smooth_X = torch.concatenate((smooth_X, sorted_smooth[j].X), axis = 0)\n",
    "                    smooth_y = torch.concatenate((smooth_y, sorted_smooth[j].y), axis = 0)\n",
    "                    post_acc_X = torch.concatenate((post_acc_X, sorted_post_acc[j].X), axis = 0)\n",
    "                    post_acc_y = torch.concatenate((post_acc_y, sorted_post_acc[j].y), axis = 0)\n",
    "                    base_acc_X = torch.concatenate((base_acc_X, sorted_base_acc[j].X), axis = 0)\n",
    "                    base_acc_y = torch.concatenate((base_acc_y, sorted_base_acc[j].y), axis = 0)\n",
    "\n",
    "                for i in range(len(top_candidate)):\n",
    "                    single_score_loss, single_score_acc, score_loss, score_acc = compute_data_err_acc(test_X, test_y, score_X[:int(top_candidate[i]*train_size)], score_y[:int(top_candidate[i]*train_size)])\n",
    "                    single_post_loss_loss, single_post_loss_acc, post_loss_loss, post_loss_acc = compute_data_err_acc(test_X, test_y, post_loss_X[:int(top_candidate[i]*train_size)], post_loss_y[:int(top_candidate[i]*train_size)])\n",
    "                    single_base_loss_loss, single_base_loss_acc, base_loss_loss, base_loss_acc = compute_data_err_acc(test_X, test_y, base_loss_X[:int(top_candidate[i]*train_size)], base_loss_y[:int(top_candidate[i]*train_size)])\n",
    "                    single_smooth_loss, single_smooth_acc, smooth_loss, smooth_acc = compute_data_err_acc(test_X, test_y, smooth_X[:int(top_candidate[i]*train_size)], smooth_y[:int(top_candidate[i]*train_size)])\n",
    "                    single_post_acc_loss, single_post_acc_acc, post_acc_loss, post_acc_acc = compute_data_err_acc(test_X, test_y, post_acc_X[:int(top_candidate[i]*train_size)], post_acc_y[:int(top_candidate[i]*train_size)])\n",
    "                    single_base_acc_loss, single_base_acc_acc, base_acc_loss, base_acc_acc = compute_data_err_acc(test_X, test_y, base_acc_X[:int(top_candidate[i]*train_size)], base_acc_y[:int(top_candidate[i]*train_size)])\n",
    "\n",
    "                    single_resi = result(single_score_loss, single_score_acc, single_post_loss_loss, single_post_loss_acc, single_base_loss_loss, single_base_loss_acc, single_smooth_loss, single_smooth_acc, single_post_acc_loss, single_post_acc_acc, single_base_acc_loss, single_base_acc_acc)\n",
    "                    resi = result(score_loss, score_acc, post_loss_loss, post_loss_acc, base_loss_loss, base_loss_acc, smooth_loss, smooth_acc, post_acc_loss, post_acc_acc, base_acc_loss, base_acc_acc)\n",
    "                    single_results[i].append(single_resi)\n",
    "                    results[i].append(resi)\n",
    "                    single_total_results[i].add(single_resi)\n",
    "                    total_results[i].add(resi)\n",
    "\n",
    "                    # print(single_resi.getacc(), single_resi.getloss(), resi.getacc(), resi.getloss())\n",
    "                    # sys.exit()\n",
    "            for i in range(len(top_candidate)):\n",
    "                single_total_results[i].divide(T)\n",
    "                total_results[i].divide(T)\n",
    "                single_summ = summary([0, 0], [0, 0], [0, 0], [0, 0], single_total_results[i])\n",
    "                summ = summary([0, 0], [0, 0], [0, 0], [0, 0], total_results[i])\n",
    "\n",
    "                for t in range(T):\n",
    "                    if single_results[i][t].smooth_acc >= single_results[i][t].post_loss_acc:\n",
    "                        single_summ.percentage_smooth_post[1] += 1/T*100\n",
    "                    if single_results[i][t].smooth_acc > single_results[i][t].post_loss_acc:\n",
    "                        single_summ.percentage_smooth_post[0] += 1/T*100\n",
    "                    if single_results[i][t].score_acc >= single_results[i][t].post_loss_acc:\n",
    "                        single_summ.percentage_score_post[1] += 1/T*100\n",
    "                    if single_results[i][t].score_acc > single_results[i][t].post_loss_acc:\n",
    "                        single_summ.percentage_score_post[0] += 1/T*100\n",
    "                    if single_results[i][t].post_loss_acc >= single_results[i][t].base_loss_acc:\n",
    "                        single_summ.percentage_post_base_loss[1] += 1/T*100\n",
    "                    if single_results[i][t].post_loss_acc > single_results[i][t].base_loss_acc:\n",
    "                        single_summ.percentage_post_base_loss[0] += 1/T*100\n",
    "                    if single_results[i][t].post_acc_acc >= single_results[i][t].base_acc_acc:\n",
    "                        single_summ.percentage_post_base_acc[1] += 1/T*100\n",
    "                    if single_results[i][t].post_acc_acc > single_results[i][t].base_acc_acc:\n",
    "                        single_summ.percentage_post_base_acc[0] += 1/T*100\n",
    "\n",
    "                    if results[i][t].smooth_acc >= results[i][t].post_loss_acc:\n",
    "                        summ.percentage_smooth_post[1] += 1/T*100\n",
    "                    if results[i][t].smooth_acc > results[i][t].post_loss_acc:\n",
    "                        summ.percentage_smooth_post[0] += 1/T*100\n",
    "                    if results[i][t].score_acc >= results[i][t].post_loss_acc:\n",
    "                        summ.percentage_score_post[1] += 1/T*100\n",
    "                    if results[i][t].score_acc > results[i][t].post_loss_acc:\n",
    "                        summ.percentage_score_post[0] += 1/T*100\n",
    "                    if results[i][t].post_loss_acc >= results[i][t].base_loss_acc:\n",
    "                        summ.percentage_post_base_loss[1] += 1/T*100\n",
    "                    if results[i][t].post_loss_acc > results[i][t].base_loss_acc:\n",
    "                        summ.percentage_post_base_loss[0] += 1/T*100\n",
    "                    if results[i][t].post_acc_acc >= results[i][t].base_acc_acc:\n",
    "                        summ.percentage_post_base_acc[1] += 1/T*100\n",
    "                    if results[i][t].post_acc_acc > results[i][t].base_acc_acc:\n",
    "                        summ.percentage_post_base_acc[0] += 1/T*100\n",
    "                single_total_summary[i].add(single_summ)\n",
    "                total_summary[i].add(summ)\n",
    "        for i in range(len(top_candidate)):\n",
    "            single_total_summary[i].divide(D)\n",
    "            total_summary[i].divide(D)\n",
    "            label = str(top_candidate[i]) + \", \" + str(test_size) + \", \" + str(num_candidate)\n",
    "            x_label.append(label)\n",
    "            single_acc = single_total_summary[i].getacc()\n",
    "            acc = total_summary[i].getacc()\n",
    "            single_loss = single_total_summary[i].getloss()\n",
    "            loss = total_summary[i].getloss()\n",
    "            print(\"$\"+label+\"$ & \" + \"{:.4f}\".format(single_acc[0]) + \" & \" + \"{:.4f}\".format(single_acc[1]) + \" & \" + \"{:.4f}\".format(single_acc[2]) + \" & \" + \"{:.4f}\".format(single_acc[3]) + \" & \" + \"{:.4f}\".format(single_acc[4]) + \" & \" + \"{:.4f}\".format(single_acc[5]) +\" \\\\\\\\ \" )\n",
    "            print(\"$\"+label+\"$ & \" + \"{:.4f}\".format(acc[0]) + \" & \" + \"{:.4f}\".format(acc[1]) + \" & \" + \"{:.4f}\".format(acc[2]) + \" & \" + \"{:.4f}\".format(acc[3]) + \" & \" + \"{:.4f}\".format(acc[4]) + \" & \" + \"{:.4f}\".format(acc[5]) +\" \\\\\\\\ \" )\n",
    "            print(\"$\"+label+\"$ & \" + \"{:.4f}\".format(single_acc[0]-acc[0]) + \" & \" + \"{:.4f}\".format(single_acc[1]-acc[1]) + \" & \" + \"{:.4f}\".format(single_acc[2]-acc[2]) + \" & \" + \"{:.4f}\".format(single_acc[3]-acc[3]) + \" & \" + \"{:.4f}\".format(single_acc[4]-acc[4]) + \" & \" + \"{:.4f}\".format(single_acc[5]-acc[5]) + \" & \" )\n",
    "            print(\"$\"+label+\"$ & \" + \"{:.4f}\".format(single_loss[0]) + \" & \" + \"{:.4f}\".format(single_loss[1]) + \" & \" + \"{:.4f}\".format(single_loss[2]) + \" & \" + \"{:.4f}\".format(single_loss[3]) + \" & \" + \"{:.4f}\".format(single_loss[4]) + \" & \" + \"{:.4f}\".format(single_loss[5]) +\" \\\\\\\\ \" )\n",
    "            print(\"$\"+label+\"$ & \" + \"{:.4f}\".format(loss[0]) + \" & \" + \"{:.4f}\".format(loss[1]) + \" & \" + \"{:.4f}\".format(loss[2]) + \" & \" + \"{:.4f}\".format(loss[3]) + \" & \" + \"{:.4f}\".format(loss[4]) + \" & \" + \"{:.4f}\".format(loss[5]) +\" \\\\\\\\ \" )\n",
    "            print(\"$\"+label+\"$ & \" + \"{:.4f}\".format(single_loss[0]-loss[0]) + \" & \" + \"{:.4f}\".format(single_loss[1]-loss[1]) + \" & \" + \"{:.4f}\".format(single_loss[2]-loss[2]) + \" & \" + \"{:.4f}\".format(single_loss[3]-loss[3]) + \" & \" + \"{:.4f}\".format(single_loss[4]-loss[4]) + \" & \" + \"{:.4f}\".format(single_loss[5]-loss[5]) + \" & \" )\n",
    "            single_smooth_post_L.append(single_total_summary[i].percentage_smooth_post[0])\n",
    "            single_smooth_post_H.append(single_total_summary[i].percentage_smooth_post[1])\n",
    "            single_score_post_L.append(single_total_summary[i].percentage_score_post[0])\n",
    "            single_score_post_H.append(single_total_summary[i].percentage_score_post[1])\n",
    "            single_post_base_loss_L.append(single_total_summary[i].percentage_post_base_loss[0])\n",
    "            single_post_base_loss_H.append(single_total_summary[i].percentage_post_base_loss[1])\n",
    "            single_post_base_acc_L.append(single_total_summary[i].percentage_post_base_acc[0])\n",
    "            single_post_base_acc_H.append(single_total_summary[i].percentage_post_base_acc[1])\n",
    "\n",
    "            smooth_post_L.append(total_summary[i].percentage_smooth_post[0])\n",
    "            smooth_post_H.append(total_summary[i].percentage_smooth_post[1])\n",
    "            score_post_L.append(total_summary[i].percentage_score_post[0])\n",
    "            score_post_H.append(total_summary[i].percentage_score_post[1])\n",
    "            post_base_loss_L.append(total_summary[i].percentage_post_base_loss[0])\n",
    "            post_base_loss_H.append(total_summary[i].percentage_post_base_loss[1])\n",
    "            post_base_acc_L.append(total_summary[i].percentage_post_base_acc[0])\n",
    "            post_base_acc_H.append(total_summary[i].percentage_post_base_acc[1])\n",
    "\n",
    "            single_acc_all.append(single_acc)\n",
    "            acc_all.append(acc)\n",
    "            single_loss_all.append(single_loss)\n",
    "            loss_all.append(loss)\n",
    "        profiler.stop()\n",
    "\n",
    "        profiler.print()\n",
    "        sys.exit()\n",
    "    \n",
    "    x_label_all += x_label\n",
    "    single_smooth_post_L_all += single_smooth_post_L\n",
    "    single_smooth_post_H_all += single_smooth_post_H\n",
    "    single_score_post_L_all += single_score_post_L\n",
    "    single_score_post_H_all += single_score_post_H\n",
    "    single_post_base_loss_L_all += single_post_base_loss_L\n",
    "    single_post_base_loss_H_all += single_post_base_loss_H\n",
    "    single_post_base_acc_L_all += single_post_base_acc_L\n",
    "    single_post_base_acc_H_all += single_post_base_acc_H\n",
    "\n",
    "    smooth_post_L_all += smooth_post_L\n",
    "    smooth_post_H_all += smooth_post_H\n",
    "    score_post_L_all += score_post_L\n",
    "    score_post_H_all += score_post_H\n",
    "    post_base_loss_L_all += post_base_loss_L\n",
    "    post_base_loss_H_all += post_base_loss_H\n",
    "    post_base_acc_L_all += post_base_acc_L\n",
    "    post_base_acc_H_all += post_base_acc_H\n",
    "\n",
    "    single_acc_all_all += single_acc_all\n",
    "    acc_all_all += acc_all\n",
    "    single_loss_all_all += single_loss_all\n",
    "    loss_all_all += loss_all\n",
    "\n",
    "    generate_plot(x_label, single_smooth_post_L, single_smooth_post_H, 'Posterior Predictive', 'Small Smooth', 'single_smooth_post')\n",
    "    generate_plot(x_label, single_score_post_L, single_score_post_H, 'Posterior Predictive', 'PMI', 'single_score_post')\n",
    "    generate_plot(x_label, single_post_base_loss_L, single_post_base_loss_H, 'Cross Entropy', 'Average Cross Entropy', 'single_post_base_loss')\n",
    "    generate_plot(x_label, single_post_base_acc_L, single_post_base_acc_H, 'Accuracy', 'Average Accuracy', 'single_post_base_acc')\n",
    "    generate_plot(x_label, smooth_post_L, smooth_post_H, 'Posterior Predictive', 'Small Smooth', 'smooth_post')\n",
    "    generate_plot(x_label, score_post_L, score_post_H, 'Posterior Predictive', 'PMI', 'score_post')\n",
    "    generate_plot(x_label, post_base_loss_L, post_base_loss_H, 'Cross Entropy', 'Average Cross Entropy', 'post_base_loss')\n",
    "    generate_plot(x_label, post_base_acc_L, post_base_acc_H, 'Accuracy', 'Average Accuracy', 'post_base_acc')\n",
    "\n",
    "# profiler.stop()\n",
    "\n",
    "# profiler.print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_plot(x_label_all, single_smooth_post_L_all, single_smooth_post_H_all, 'Posterior Predictive', 'Small Smooth', 'single_smooth_post_all')\n",
    "generate_plot(x_label_all, single_score_post_L_all, single_score_post_H_all, 'Posterior Predictive', 'PMI', 'single_score_post_all')\n",
    "generate_plot(x_label_all, single_post_base_loss_L_all, single_post_base_loss_H_all, 'Cross Entropy', 'Average Cross Entropy', 'single_post_base_loss_all')\n",
    "generate_plot(x_label_all, single_post_base_acc_L_all, single_post_base_acc_H_all, 'Accuracy', 'Average Accuracy', 'single_post_base_acc_all')\n",
    "generate_plot(x_label_all, smooth_post_L_all, smooth_post_H_all, 'Posterior Predictive', 'Small Smooth', 'smooth_post_all')\n",
    "generate_plot(x_label_all, score_post_L_all, score_post_H_all, 'Posterior Predictive', 'PMI', 'score_post_all')\n",
    "generate_plot(x_label_all, post_base_loss_L_all, post_base_loss_H_all, 'Cross Entropy', 'Average Cross Entropy', 'post_base_loss_all')\n",
    "generate_plot(x_label_all, post_base_acc_L_all, post_base_acc_H_all, 'Accuracy', 'Average Accuracy', 'post_base_acc_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(acc_all_all)\n",
    "# print(single_total_summary[2].total_result.score_loss)\n",
    "# print(single_total_summary[2].total_result.score_acc)\n",
    "# print(single_total_summary[2].total_result.err_loss)\n",
    "# print(single_total_summary[2].total_result.err_acc)\n",
    "# print(single_total_summary[2].total_result.post_loss)\n",
    "# print(single_total_summary[2].total_result.post_acc)\n",
    "# print(single_total_summary[2].total_result.acc_loss)\n",
    "# print(single_total_summary[2].total_result.acc_acc)\n",
    "# print(single_total_summary[2].total_result.smooth_loss)\n",
    "# print(single_total_summary[2].total_result.smooth_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profiler.stop()\n",
    "\n",
    "# profiler.print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
